# ğŸš€ Multi-Modal Retrieval-Augmented Generation (RAG) System

Production-oriented **multi-source RAG** application that ingests documents, web pages, videos & audio â†’ builds a persistent vector knowledge base â†’ and answers questions grounded **strictly** in the provided content using a powerful LLM.

[![Untitled-Diagram-drawio.png](https://i.postimg.cc/1tzQ863C/Untitled-Diagram-drawio.png)](https://postimg.cc/R3288WfK)

**Focus:** System design â€¢ Correct data flow â€¢ Real RAG engineering â€” not just calling an LLM API.

---

## âœ¨ Key Features

- ğŸ“‚ **Multi-modal ingestion** â€” PDFs â€¢ Text â€¢ Word â€¢ Web URLs â€¢ YouTube/general videos â€¢ Audio (transcription)
- âœ‚ï¸ **Intelligent semantic chunking** â€” `RecursiveCharacterTextSplitter`
- ğŸ§  **High-quality embeddings** â€” Sentence Transformers (HuggingFace)
- âš¡ **Fast & lightweight vector search** â€” FAISS
- ğŸ” **Multi-contextual retriever** with metadata filtering support
- ğŸ¤– **Grounded generation** â€” Qwen 30B (via Groq or local)
- ğŸŒ **Clean Flask web UI** for upload + querying
- ğŸ§± **Modular, maintainable, scalable architecture**

---

## ğŸ§  High-Level Architecture

```mermaid
graph TD
    A[User Input: PDF / URL / Video / Audio] --> B[Loader Layer]
    B --> C[RecursiveCharacterTextSplitter]
    C --> D[Chunked Documents]
    D --> E[Sentence Transformer Embedder]
    E --> F[FAISS Vector Store]
    F --> G[Query â†’ Embed â†’ Retrieve top-K]
    G --> H[Context + Query â†’ Prompt]
    H --> I[Qwen 30B LLM]
    I --> J[Grounded, Cited Answer]
```

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ app.py                      # Flask entry point
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ query.html              # Query + upload UI
â”‚   â””â”€â”€ results.html            # Answer + sources display
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ ingestion/              # PDF, web, YouTube, audio loaders
â”‚   â”œâ”€â”€ embedding.py            # Embedder wrapper
â”‚   â”œâ”€â”€ faiss_db.py             # FAISS vector store management
â”‚   â”œâ”€â”€ retriever.py            # Retrieval logic + re-ranking (optional)
â”‚   â””â”€â”€ gen.py                  # LLM prompt + generation
â”œâ”€â”€ utility/
â”‚   â”œâ”€â”€ logger.py
â”‚   â””â”€â”€ error_handling.py
â”œâ”€â”€ vector_db/                  # (gitignored) persisted FAISS index
â”œâ”€â”€ .env
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ”„ End-to-End Pipeline

### 1ï¸âƒ£ User Input
- Upload files (PDF, docx, txtâ€¦) or paste URL(s)

### 2ï¸âƒ£ Ingestion & Processing
- Type-specific loader
- Text extraction / transcription
- `RecursiveCharacterTextSplitter` (with overlap)

### 3ï¸âƒ£ Embedding
- `all-MiniLM-L6-v2` (normalized embeddings)

### 4ï¸âƒ£ Storage
- FAISS `IndexFlatL2` (cosine similarity via normalization)
- Persistent `save_local()` / `load_local()`

### 5ï¸âƒ£ Query Flow
1. Query â†’ embed â†’ FAISS â†’ top-K chunks
2. Enrich context with source metadata
3. Strict system prompt â†’ Qwen-32B
4. **Output:** Answer strictly grounded in retrieved content
5. *(Planned: chunk-level citations & highlighting)*

---

## ğŸ›¡ï¸ Critical Engineering Lessons

| Issue | Symptom | Solution |
|-------|---------|----------|
| Multiple FAISS instances | Retrieval returns no results | Single shared instance (module-level or DI) |
| Knowledge lost on restart | DB empty after app restart | Explicit save/load logic at startup |
| Embedding model mismatch | Dimension error on load | Same embedder instance used for save & load |
| Double initialization / slow boot | Duplicate model loading | Controlled boot sequence or lazy properties |

---

## ğŸš€ Quick Start

```bash
# 1. Clone & enter directory
git clone <repository-url>
cd <project-directory>

# 2. Install dependencies
pip install -r requirements.txt

# 3. (Recommended) Create .env file
echo "GROQ_API_KEY=gsk_xxxxxxxxxxxxxxxxxxxxxxxx" > .env

# 4. Run the app
python app.py
```

**â†’ Open:** `http://127.0.0.1:5000`

---

## ğŸ›  Tech Stack

| Component | Technology |
|-----------|------------|
| **Web Framework** | Flask |
| **Embeddings** | sentence-transformers (`all-MiniLM-L6-v2`) |
| **Vector Store** | FAISS |
| **LLM** | Qwen / qwen3-32b (Groq API) |
| **Document Loaders** | LangChain loaders + yt-dlp / whisper (planned) |
| **Frontend** | Jinja2 + Bootstrap |

---

## ğŸŒŸ Roadmap / Future Enhancements

- [ ] Async / background ingestion pipeline
- [ ] User/session-based vector namespaces
- [ ] Source citations + chunk highlighting
- [ ] Streaming LLM responses
- [ ] FastAPI + WebSocket upgrade
- [ ] Hybrid retrieval (BM25 + semantic)
- [ ] RAG evaluation (RAGAS, faithfulness, answer relevance)

---


---

## ğŸ“Œ Final Note


If you understand the architecture and trade-offs in this project, you understand how modern RAG systems are actually built in 2025â€“2026.

**Happy building! ğŸš€**

---

## ğŸ“„ License

*MIT*

## ğŸ¤ Contributing

Contributions, issues, and feature requests are welcome!

---

**â­ If you find this project helpful, please give it a star!**